{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Pipeline\n",
    "Creating a Pipeline to test out each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "Option to use either Dataset by just commenting out the undesired one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 features Dataset\n",
    "with open(\"Top-10-Features-Models/top10_df.pkl\",'rb') as fp:\n",
    "    df = pickle.load(fp)\n",
    "    \n",
    "# Top 10 Correlated Dataset\n",
    "# with open(\"Top-10-Correlation-Models/top10_corr_df.pkl\",'rb') as fp:\n",
    "#     df = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "features_df = df.drop([\"Decision\"], 1)\n",
    "\n",
    "scaled_df = pd.DataFrame(scaler.fit_transform(features_df), \n",
    "                               index=features_df.index, \n",
    "                               columns=features_df.columns)\n",
    "\n",
    "df = scaled_df.join(df.Decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"Decision\"], 1)\n",
    "y = df.Decision\n",
    "\n",
    "# Train, test, split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Pipeline \n",
    "Using 10 Different Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the 10 models\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "## Preventing error from occuring: XGBoost causes kernel to die.\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating pipelines for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost\n",
    "pipe_ada = Pipeline([('clf', AdaBoostClassifier())])\n",
    "\n",
    "# Gradient Boost\n",
    "pipe_gb  = Pipeline([('clf', GradientBoostingClassifier())])\n",
    "\n",
    "# Random Forest\n",
    "pipe_rf  = Pipeline([('clf', RandomForestClassifier())])\n",
    "\n",
    "# Decision Tree\n",
    "pipe_dt  = Pipeline([('clf', DecisionTreeClassifier())])\n",
    "\n",
    "# Dummy (Baseline)\n",
    "pipe_dum = Pipeline([('clf', DummyClassifier())])\n",
    "\n",
    "# K Nearest Neighbors\n",
    "pipe_knn = Pipeline([('clf', KNeighborsClassifier())])\n",
    "\n",
    "# Logistic Regression\n",
    "pipe_lr  = Pipeline([('clf', LogisticRegression())])\n",
    "\n",
    "# Naive Bayes\n",
    "pipe_nb  = Pipeline([('clf', GaussianNB())])\n",
    "\n",
    "# Support Vector Machine\n",
    "pipe_svm = Pipeline([('clf', SVC())])\n",
    "\n",
    "# XGBoost\n",
    "pipe_xgb = Pipeline([('clf', XGBClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a List of Model Names and Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [pipe_ada, pipe_gb, pipe_rf, pipe_dt, pipe_dum, pipe_knn, pipe_lr, pipe_nb, pipe_svm, pipe_xgb]\n",
    "\n",
    "models = ['Adaboost', \n",
    "          'GradientBoost', \n",
    "          'RandomForest', \n",
    "          'DecisionTree', \n",
    "          'Dummy(Baseline)', \n",
    "          'KNN', \n",
    "          'LogisticRegression',\n",
    "          'NaiveBayes',\n",
    "          'SupportVectorMachine',\n",
    "          'XGBoost']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and Training each Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('clf', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('clf', GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_...    subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('clf', DummyClassifier(constant=None, random_state=None, strategy='stratified'))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('clf', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform'))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('clf', GaussianNB(priors=None, var_smoothing=1e-09))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('clf', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('clf', XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None,\n",
      "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
      "       subsample=1, verbosity=1))])\n"
     ]
    }
   ],
   "source": [
    "# Looping through each Pipeline to fit and train each model\n",
    "for pipe in pipelines:\n",
    "    print(pipe)\n",
    "    pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report for each Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adaboost - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Sell       0.35      0.04      0.08      2898\n",
      "         Buy       0.47      0.35      0.40      5509\n",
      "        Hold       0.46      0.74      0.57      6636\n",
      "\n",
      "   micro avg       0.46      0.46      0.46     15043\n",
      "   macro avg       0.43      0.38      0.35     15043\n",
      "weighted avg       0.44      0.46      0.41     15043\n",
      "\n",
      "\n",
      "GradientBoost - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Sell       0.35      0.03      0.06      2898\n",
      "         Buy       0.48      0.34      0.39      5509\n",
      "        Hold       0.46      0.76      0.57      6636\n",
      "\n",
      "   micro avg       0.46      0.46      0.46     15043\n",
      "   macro avg       0.43      0.38      0.34     15043\n",
      "weighted avg       0.45      0.46      0.41     15043\n",
      "\n",
      "\n",
      "RandomForest - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Sell       0.27      0.19      0.22      2898\n",
      "         Buy       0.41      0.45      0.43      5509\n",
      "        Hold       0.46      0.48      0.47      6636\n",
      "\n",
      "   micro avg       0.41      0.41      0.41     15043\n",
      "   macro avg       0.38      0.37      0.37     15043\n",
      "weighted avg       0.40      0.41      0.41     15043\n",
      "\n",
      "\n",
      "DecisionTree - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Sell       0.23      0.24      0.24      2898\n",
      "         Buy       0.39      0.39      0.39      5509\n",
      "        Hold       0.45      0.44      0.45      6636\n",
      "\n",
      "   micro avg       0.38      0.38      0.38     15043\n",
      "   macro avg       0.36      0.36      0.36     15043\n",
      "weighted avg       0.39      0.38      0.39     15043\n",
      "\n",
      "\n",
      "Dummy(Baseline) - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Sell       0.18      0.17      0.18      2898\n",
      "         Buy       0.37      0.37      0.37      5509\n",
      "        Hold       0.44      0.44      0.44      6636\n",
      "\n",
      "   micro avg       0.37      0.37      0.37     15043\n",
      "   macro avg       0.33      0.33      0.33     15043\n",
      "weighted avg       0.36      0.37      0.36     15043\n",
      "\n",
      "\n",
      "KNN - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Sell       0.23      0.24      0.24      2898\n",
      "         Buy       0.40      0.44      0.42      5509\n",
      "        Hold       0.46      0.41      0.43      6636\n",
      "\n",
      "   micro avg       0.39      0.39      0.39     15043\n",
      "   macro avg       0.36      0.36      0.36     15043\n",
      "weighted avg       0.39      0.39      0.39     15043\n",
      "\n",
      "\n",
      "LogisticRegression - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Sell       0.00      0.00      0.00      2898\n",
      "         Buy       0.43      0.05      0.08      5509\n",
      "        Hold       0.44      0.97      0.61      6636\n",
      "\n",
      "   micro avg       0.44      0.44      0.44     15043\n",
      "   macro avg       0.29      0.34      0.23     15043\n",
      "weighted avg       0.35      0.44      0.30     15043\n",
      "\n",
      "\n",
      "NaiveBayes - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Sell       0.22      0.06      0.09      2898\n",
      "         Buy       0.37      0.95      0.53      5509\n",
      "        Hold       0.38      0.00      0.01      6636\n",
      "\n",
      "   micro avg       0.36      0.36      0.36     15043\n",
      "   macro avg       0.32      0.34      0.21     15043\n",
      "weighted avg       0.34      0.36      0.21     15043\n",
      "\n",
      "\n",
      "SupportVectorMachine - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Sell       0.17      0.00      0.00      2898\n",
      "         Buy       0.44      0.05      0.09      5509\n",
      "        Hold       0.44      0.97      0.61      6636\n",
      "\n",
      "   micro avg       0.44      0.44      0.44     15043\n",
      "   macro avg       0.35      0.34      0.23     15043\n",
      "weighted avg       0.39      0.44      0.30     15043\n",
      "\n",
      "\n",
      "XGBoost - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Sell       0.38      0.03      0.05      2898\n",
      "         Buy       0.48      0.34      0.40      5509\n",
      "        Hold       0.46      0.76      0.58      6636\n",
      "\n",
      "   micro avg       0.47      0.47      0.47     15043\n",
      "   macro avg       0.44      0.38      0.34     15043\n",
      "weighted avg       0.45      0.47      0.41     15043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, val in enumerate(pipelines):\n",
    "    print('\\n'+ models[index] + ' -'*30)\n",
    "    \n",
    "    report = classification_report(y_test, val.predict(X_test), target_names=['Sell', 'Buy', 'Hold'])\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
