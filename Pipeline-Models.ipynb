{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Pipeline\n",
    "Creating a Pipeline to test out each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "Option to use either Dataset by just commenting out the undesired one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 features Dataset\n",
    "with open(\"Top-10-Features-Models/top10_df.pkl\",'rb') as fp:\n",
    "    df = pickle.load(fp)\n",
    "    \n",
    "# Top 10 Correlated Dataset\n",
    "# with open(\"Top-10-Correlation-Models/top10_corr_df.pkl\",'rb') as fp:\n",
    "#     df = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "features_df = df.drop([\"Decision\"], 1)\n",
    "\n",
    "scaled_df = pd.DataFrame(scaler.fit_transform(features_df), \n",
    "                               index=features_df.index, \n",
    "                               columns=features_df.columns)\n",
    "\n",
    "df = scaled_df.join(df.Decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"Decision\"], 1)\n",
    "y = df.Decision\n",
    "\n",
    "# Train, test, split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Pipeline \n",
    "Using 10 Different Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the 10 models\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "## Preventing error from occuring: XGBoost causes kernel to die.\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating pipelines for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost\n",
    "pipe_ada = Pipeline([('clf', AdaBoostClassifier())])\n",
    "\n",
    "# Gradient Boost\n",
    "pipe_gb  = Pipeline([('clf', GradientBoostingClassifier())])\n",
    "\n",
    "# Random Forest\n",
    "pipe_rf  = Pipeline([('clf', RandomForestClassifier())])\n",
    "\n",
    "# Decision Tree\n",
    "pipe_dt  = Pipeline([('clf', DecisionTreeClassifier())])\n",
    "\n",
    "# Dummy (Baseline)\n",
    "pipe_dum = Pipeline([('clf', DummyClassifier())])\n",
    "\n",
    "# K Nearest Neighbors\n",
    "pipe_knn = Pipeline([('clf', KNeighborsClassifier())])\n",
    "\n",
    "# Logistic Regression\n",
    "pipe_lr  = Pipeline([('clf', LogisticRegression())])\n",
    "\n",
    "# Naive Bayes\n",
    "pipe_nb  = Pipeline([('clf', GaussianNB())])\n",
    "\n",
    "# Support Vector Machine\n",
    "pipe_svm = Pipeline([('clf', SVC())])\n",
    "\n",
    "# XGBoost\n",
    "pipe_xgb = Pipeline([('clf', XGBClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a List of Model Names and Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [pipe_ada, pipe_gb, pipe_rf, pipe_dt, pipe_dum, pipe_knn, pipe_lr, pipe_nb, pipe_svm, pipe_xgb]\n",
    "\n",
    "models = ['Adaboost', \n",
    "          'GradientBoost', \n",
    "          'RandomForest', \n",
    "          'DecisionTree', \n",
    "          'Dummy(Baseline)', \n",
    "          'KNN', \n",
    "          'LogisticRegression',\n",
    "          'NaiveBayes',\n",
    "          'SupportVectorMachine',\n",
    "          'XGBoost']\n",
    "\n",
    "# Zipping the the strings and pipelines together and creating a dictionary\n",
    "model_pipelines = dict(zip(models, pipelines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and Training each Pipeline\n",
    "(Using default parameters initially)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('clf', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('clf', GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_...    subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('clf', DummyClassifier(constant=None, random_state=None, strategy='stratified'))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('clf', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform'))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('clf', GaussianNB(priors=None, var_smoothing=1e-09))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('clf', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('clf', XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None,\n",
      "       objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
      "       subsample=1, verbosity=1))])\n"
     ]
    }
   ],
   "source": [
    "# Looping through each Pipeline to fit and train each model\n",
    "for name, pipe in model_pipelines.items():\n",
    "    print(pipe)\n",
    "    pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report for each Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adaboost (Macro Avg - F1 Score):\n",
      "0.34822250432853535\n",
      "\n",
      "GradientBoost (Macro Avg - F1 Score):\n",
      "0.3431787749786248\n",
      "\n",
      "RandomForest (Macro Avg - F1 Score):\n",
      "0.3756400865302727\n",
      "\n",
      "DecisionTree (Macro Avg - F1 Score):\n",
      "0.36039506834044205\n",
      "\n",
      "Dummy(Baseline) (Macro Avg - F1 Score):\n",
      "0.3336222492246383\n",
      "\n",
      "KNN (Macro Avg - F1 Score):\n",
      "0.36300443032863144\n",
      "\n",
      "LogisticRegression (Macro Avg - F1 Score):\n",
      "0.2305696283747186\n",
      "\n",
      "NaiveBayes (Macro Avg - F1 Score):\n",
      "0.20873732913818646\n",
      "\n",
      "SupportVectorMachine (Macro Avg - F1 Score):\n",
      "0.2319655621129266\n",
      "\n",
      "XGBoost (Macro Avg - F1 Score):\n",
      "0.34300840024402146\n"
     ]
    }
   ],
   "source": [
    "# Looping through each model's predictions and getting their classification reports\n",
    "for name, pipe in model_pipelines.items():\n",
    "    print('\\n'+ name + ' (Macro Avg - F1 Score):')\n",
    "    \n",
    "    report = classification_report(y_test, pipe.predict(X_test), target_names=['Sell', 'Buy', 'Hold'], output_dict=True)\n",
    "    print(report['macro avg']['f1-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the best performing model based on the __Macro Average for the F1 Score__.  This is due to the fact that we want to optimize the amount of classifications (_Recall_) and reduce the amount of misclassifications (_Precision_).  _Macro Average_ is used because of the class imbalance and the desire to classify more 'Buys' and 'Sells' than 'Holds'.\n",
    "\n",
    "However, in the end, we will be evaluating on _Precision_ Score for Macro Average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 3 Classifiers for GridSearch\n",
    "1. Random Forest\n",
    "2. KNN\n",
    "3. Decision Tree\n",
    "\n",
    "_(Based on Macro Avg F1-Score)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Grid Search\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('RandomForest', 'KNN')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-5bcce121b741>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtop3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_pipelines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RandomForest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KNN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: ('RandomForest', 'KNN')"
     ]
    }
   ],
   "source": [
    "top3 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
